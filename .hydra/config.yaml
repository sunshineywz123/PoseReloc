work_dir: ${hydra:runtime.cwd}
data_dir: ${work_dir}/data
print_config: true
seed: 12345
task_name: loftr_train
exp_name: hello_world
data_base_dir: /nas/users/hexingyi/onepose_hard_data
sfm_dir: /nas/users/hexingyi/onepose_sfm
merge_output_dir: /nas/users/hexingyi/onepose_merged
trainer:
  _target_: pytorch_lightning.Trainer
  gpus:
  - 1
  min_epochs: 1
  max_epochs: 15
  gradient_clip_val: 0.5
  accumulate_grad_batches: 2
  weights_summary: null
  num_sanity_val_steps: 2
model:
  _target_: src.models.GATs_LoFTR_lightning_model.PL_GATsLoFTR
  optimizer: adam
  lr: 0.0001
  weight_decay: 0.0
  architecture: loftr
  pretrained_ckpt: null
  loftr:
    loftr_backbone:
      type: ResNetFPN
      resolution:
      - 8
      - 2
      resnetfpn:
        block_type: BasicBlock
        initial_dim: 128
        block_dims:
        - 128
        - 196
        - 256
        output_layers:
        - 3
        - 1
      pretrained: weight/loftr_w9_no_cat_coarse_auc10=0.685.ckpt
    use_fine_backbone_as_coarse: true
    interpol_type: nearest
    keypoints_encoding:
      descriptor_dim: 128
      keypoints_encoder:
      - 32
      - 64
      - 128
    loftr_coarse:
      type: LoFTR
      d_model: 128
      d_ffm: 128
      nhead: 8
      layer_names:
      - GATs
      - self
      - cross
      layer_iter_n: 4
      dropout: 0.0
      attention: linear
      kernel_fn: elu + 1
      d_kernel: 16
      redraw_interval: 2
      rezero: null
      final_proj: false
      pos_emb_shape:
      - 128
      - 128
      d_db_feat: 128
      GATs_dropout: 0.6
      GATs_alpha: 0.2
    coarse_matching:
      type: dual-softmax
      thr: 0.2
      feat_norm_method: sqrt_feat_dim
      border_rm: 2
      train:
        train_padding: false
        train_coarse_percent: 0.4
        train_pad_num_gt_min: 200
      dual_softmax:
        temperature: 0.1
    loftr_fine:
      enable: false
  loss:
    coarse_type: focal
    coarse_weight: 1.0
    fine_type: l2_with_std
    fine_wight: 1.0
    focal_alpha: 0.25
    focal_gamma: 2.0
    pos_weight: 1.0
    neg_weight: 1.0
    fine_smooth_l1_beta: 1.0
    fine_loss_weight: 1.0
    fine_correct_thr: 1.0
  trainer:
    canonical_bs: 4
    canoncial_lr: 0.0001
    scaling: null
    optimizer: adamw
    true_lr: 0.0001
    adam_decay: 0.0
    adamw_decay: 0.1
    scheduler: MultiStepLR
    scheduler_invervel: epoch
    mslr_milestones:
    - 3
    - 6
    - 9
    - 12
    mslr_gamma: 0.5
    cosa_tmax: 30
    elr_gamma: 0.999992
  descriptor_dim: 256
  keypoints_encoder:
  - 32
  - 64
  - 128
  sinkhorn_iterations: 100
  match_threshold: 0.2
  match_type: softmax
  milestones:
  - 2
  - 5
  - 7
  - 10
  gamma: 0.5
  pos_weights: 0.5
  neg_weights: 0.5
datamodule:
  _target_: src.datamodules.GATs_loftr_datamodule.GATsLoFTRDataModule
  data_dirs: ${sfm_dir}
  anno_dirs: outputs_${model.match_type}/anno
  anno_file: ${merge_output_dir}/${task_name}/train.json
  batch_size: 1
  num_workers: 4
  pin_memory: true
  num_leaf: 5
  shape2d: 2000
  shape3d: 10000
  img_pad: false
  img_resize:
  - 512
  - 512
  df: 8
  coarse_scale: 0.125
callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val/loss
    save_top_k: -1
    save_last: true
    mode: min
    dirpath: ${data_dir}/models/checkpoints/${exp_name}
    filename: '{epoch}'
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
logger:
  tensorboard:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ${data_dir}/logs
    name: ${exp_name}
    default_hp_metric: false
  neptune:
    tags:
    - best_model
  csv_logger:
    save_dir: .
