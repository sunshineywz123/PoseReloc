work_dir: ${hydra:runtime.cwd}
data_dir: ${work_dir}/data
print_config: true
seed: 12345
task_name: loftr_train
exp_name: loftr_train
data_base_dir: /nas/users/hexingyi/onepose_hard_data
sfm_dir: /nas/users/hexingyi/onepose_sfm
merge_output_dir: /nas/users/hexingyi/onepose_merged
trainer:
  _target_: pytorch_lightning.Trainer
  gpus:
  - 3
  num_nodes: 1
  accelerator: ddp
  min_epochs: 1
  max_epochs: 15
  gradient_clip_val: 0.5
  accumulate_grad_batches: 2
  weights_summary: null
  num_sanity_val_steps: 2
  check_val_every_n_epoch: 1
  log_every_n_steps: 70
  flush_logs_every_n_steps: 1
model:
  _target_: src.models.GATs_LoFTR_lightning_model.PL_GATsLoFTR
  pretrained_ckpt: null
  loftr:
    loftr_backbone:
      type: ResNetFPN
      resolution:
      - 8
      - 2
      resnetfpn:
        block_type: BasicBlock
        initial_dim: 128
        block_dims:
        - 128
        - 196
        - 256
        output_layers:
        - 3
        - 1
      pretrained: weight/loftr_w9_no_cat_coarse_auc10=0.685.ckpt
      pretrained_fix: false
    use_fine_backbone_as_coarse: true
    interpol_type: bilinear
    keypoints_encoding:
      enable: true
      descriptor_dim: 128
      keypoints_encoder:
      - 32
      - 64
      - 128
      norm_method: instancenorm
    positional_encoding:
      enable: true
      pos_emb_shape:
      - 256
      - 256
    loftr_coarse:
      type: LoFTR
      d_model: 128
      d_ffm: 128
      nhead: 4
      layer_names:
      - self
      - cross
      layer_iter_n: 4
      dropout: 0.0
      attention: linear
      norm_method: layernorm
      kernel_fn: elu + 1
      d_kernel: 16
      redraw_interval: 2
      rezero: null
      final_proj: false
      d_db_feat: 128
      GATs_dropout: 0.6
      GATs_alpha: 0.2
    coarse_matching:
      type: dual-softmax
      thr: 0.2
      feat_norm_method: sqrt_feat_dim
      border_rm: 2
      spg_spvs: false
      skh:
        fp16: false
        iters: 20
        partial_impl: dustbin
        init_bin_score: 1.0
        prototype_impl: learned
        prefilter: true
        with_prior: false
        linear:
          enable: false
          mapping: favor
          mapping_dim: 4096
      dual_softmax:
        temperature: 0.1
        temperature_learnable: false
        range:
        - 0.03
        - 0.8
      train:
        train_padding: false
        train_coarse_percent: 0.4
        train_pad_num_gt_min: 200
    loftr_fine:
      enable: false
  loss:
    spg_spvs: false
    coarse_type: focal
    coarse_weight: 1.0
    fine_type: l2_with_std
    fine_wight: 1.0
    focal_alpha: 0.5
    focal_gamma: 1.0
    pos_weight: 1.0
    neg_weight: 1.0
    fine_smooth_l1_beta: 1.0
    fine_loss_weight: 1.0
    fine_correct_thr: 1.0
  trainer:
    enable_plotting: true
    canonical_bs: 64
    canonical_lr: 0.04
    scaling: null
    world_size: null
    n_val_pairs_to_plot: 32
    optimizer: adamw
    true_lr: null
    adam_decay: 0.0
    adamw_decay: 0.1
    scheduler: MultiStepLR
    scheduler_invervel: epoch
    mslr_milestones:
    - 3
    - 6
    - 9
    - 12
    mslr_gamma: 0.5
    cosa_tmax: 30
    elr_gamma: 0.999992
  eval_metrics:
    point_cloud_rescale: 1000
    pnp_reprojection_error: 5
    pose_thresholds:
    - 1
    - 3
    - 5
  match_type: softmax
datamodule:
  _target_: src.datamodules.GATs_loftr_datamodule.GATsLoFTRDataModule
  data_dirs: ${sfm_dir}
  anno_dirs: outputs_${model.match_type}/anno
  train_anno_file: ${merge_output_dir}/${task_name}/train.json
  val_anno_file: ${merge_output_dir}/${task_name}/val.json
  batch_size: 1
  num_workers: 4
  pin_memory: true
  train_percent: 1.0
  val_percent: 0.4
  num_leaf: 1
  shape2d: 2000
  shape3d: 10000
  img_pad: false
  img_resize:
  - 512
  - 512
  df: 8
  coarse_scale: 0.125
callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: None
    save_top_k: -1
    save_last: true
    mode: min
    dirpath: ${work_dir}/models/checkpoints/${exp_name}
    filename: '{epoch}'
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
logger:
  tensorboard:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ${work_dir}/logs
    name: ${exp_name}
    default_hp_metric: false
  neptune:
    tags:
    - best_model
  csv_logger:
    save_dir: .
