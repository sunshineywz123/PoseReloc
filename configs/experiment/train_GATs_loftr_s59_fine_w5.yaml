# @package _global_

# to execute this experiment run:
# python train.py +experiment=PoseReloc

defaults:
    - override /trainer: null  # override trainer to null so it's not loaded from main config defaults...
    - override /model: null
    - override /datamodule: null
    - override /callbacks: null
    - override /logger: null

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config, 
# so everything is stored in one place for more readibility

seed: 12345
task_name: null
exp_name: train_coarse_thr_0.4_layernorm_n_head_4_finew5_fix_backbone_no_hard_train

# data_base_dir: '/cephfs-mvs/3dv-research/hexingyi/scan_data'
# sfm_dir: '/cephfs-mvs/3dv-research/hexingyi/onepose_sfm'
merge_output_dir: '/cephfs-mvs/3dv-research/hexingyi/onepose_merged'

trainer:
    _target_: pytorch_lightning.Trainer
    gpus: 
        - 0
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
    num_nodes: 1
    accelerator: 'ddp'

    min_epochs: 1
    max_epochs: 25
    gradient_clip_val: 0.5
    accumulate_grad_batches: 2
    weights_summary: null
    num_sanity_val_steps: 5 # 0 means no sanity check
    check_val_every_n_epoch: 1
    log_every_n_steps: 70
    flush_logs_every_n_steps: 1
    
    # resume_from_checkpoint: '${work_dir}/models/checkpoints/${exp_name}/last.ckpt'

model:
  # _target_: src.models.spg_model.LitModelSPG
  _target_: src.models.GATs_LoFTR_lightning_model.PL_GATsLoFTR
  pretrained_ckpt: null

  loftr:
    loftr_backbone:
      type: "ResNetFPN"
      resolution: [8, 2]
      resnetfpn:
        block_type: "BasicBlock"
        initial_dim: 128
        block_dims: [128, 196, 256]
        output_layers: [3, 1]

      pretrained: "weight/loftr_w9_no_cat_coarse_auc10=0.685.ckpt"
      pretrained_fix: True

    use_fine_backbone_as_coarse: True
    interpol_type: "bilinear" # ['nearest', 'bilinear']

    keypoints_encoding:
      enable: True
      descriptor_dim: 128
      keypoints_encoder: [32, 64, 128]
      norm_method: "instancenorm"

    positional_encoding:
      enable: True
      pos_emb_shape: [256, 256]
    loftr_coarse:
      type: "LoFTR"
      # type: 'LoFTR-Conv1d'
      d_model: 128
      d_ffm: 128
      nhead: 4
      # nhead: 8
      # layer_names: ['GATs', 'self', 'cross']
      layer_names: ["self", "cross"]
      layer_iter_n: 4
      dropout: 0.
      attention: "linear"
      norm_method: "layernorm"
      # norm_method: 'instancenorm'

      kernel_fn: "elu + 1" # ['elu + 1', 'Favor', 'GeneralizedRandomFeatures']
      d_kernel: 16 # (128 / 8)
      redraw_interval: 2
      rezero: null
      final_proj: False

      # GATs part
      d_db_feat: 128 # The input dim of database feature
      GATs_dropout: 0.6
      GATs_alpha: 0.2
      GATs_enable_feed_forward: False
      GATs_feed_forward_norm_method: 'instancenorm'

    coarse_matching:
      type: "dual-softmax"
      # type: 'sinkhorn'
      thr: 0.4
      feat_norm_method: "sqrt_feat_dim"
      border_rm: 2
      spg_spvs: False

      skh:
        fp16: False
        iters: 20
        partial_impl: "dustbin"
        init_bin_score: 1.0
        prototype_impl: "learned"
        prefilter: True
        with_prior: False
        linear:
          enable: False
          mapping: "favor"
          mapping_dim: 4096

      dual_softmax:
        temperature: 0.1
        temperature_learnable: False
        range: # may be log range or exp range?
          - 0.03 # min
          - 0.8 # max

      train:
        train_padding: True # Good to be true
        # From LoFTR:
        train_coarse_percent: 0.3 # save GPU memory
        train_pad_num_gt_min: 200 # avoid deadlock; better convergence

    loftr_fine:
      enable: True
      # Fine preprocess:
      window_size: 5
      concat_coarse_feat: False
      concat_coarse_feat_type: "nearest"
      coarse_layer_norm: False
      ms_feat: False
      ms_feat_type: 'PROJ_MERGE'

      # Fine module
      type: "LoFTR"
      d_model: 128
      nhead: 4
      layer_names: ["self", "cross"]
      layer_iter_n: 1
      dropout: 0.0
      attention: "linear"
      norm_method: layernorm

      kernel_fn: "elu + 1"
      d_kernel: 16
      redraw_interval: 2
      rezero: null
      final_proj: False

      # GATs part
      d_db_feat: 128 # The input dim of database feature
      GATs_dropout: 0.6
      GATs_alpha: 0.2
      GATs_enable_feed_forward: False
      GATs_feed_forward_norm_method: 'instancenorm'

    fine_matching:
        enable: True
        type: 's2d'
        detector: 'OnGrid'

        s2d:
            type: 'heatmap'

  loss:
    spg_spvs: False
    coarse_type: "focal"
    coarse_weight: 1.0
    fine_type: "l2_with_std"
    fine_weight: 0.81 # Calculated according to fine window size in train_gats_loftr.py

    # Config for coarse
    focal_alpha: 0.5 # 0.25 default
    focal_gamma: 2.0
    pos_weight: 1.0
    neg_weight: 1.0

    # smooth_l1_with_std
    fine_smooth_l1_beta: 1.0
    fine_loss_weight: 1.0
    fine_correct_thr: 1.0

  trainer:
    enable_plotting: True
    canonical_bs: 4
    canonical_lr: 2e-4 # Original: 5e-4
    scaling: null
    world_size: null
    n_val_pairs_to_plot: 100

    # Optimizer
    optimizer: "adamw" # ['adam', 'adamw']
    true_lr: null
    adam_decay: 0.
    adamw_decay: 0.1

    # Scheduler
    scheduler: "MultiStepLR"
    scheduler_invervel: "epoch"
    mslr_milestones: [3,6,9,12]
    mslr_gamma: 0.5
    cosa_tmax: 30
    elr_gamma: 0.999992

  eval_metrics:
    point_cloud_rescale: 1000
    pnp_reprojection_error: 5
    pose_thresholds: [1, 3, 5]

  match_type: "softmax"

datamodule:
    _target_: src.datamodules.GATs_loftr_datamodule.GATsLoFTRDataModule
    # data_dirs: ${sfm_dir}
    anno_dirs: outputs_${model.match_type}/anno
    train_anno_file: ${merge_output_dir}/${task_name}/train.json
    val_anno_file: ${merge_output_dir}/${task_name}/val.json

    batch_size: 4
    num_workers: 4
    pin_memory: True

    train_percent: 1.0 # For debug
    val_percent: 0.2

    # TODO: move to a template
    # 3D part
    num_leaf: 1
    shape2d: 2000 # Not used
    shape3d_train: 11000 # NOTE: Changed from 11000 to 12000
    shape3d_val: 11000

    # 2D part
    img_pad: False
    img_resize: [512, 512]
    df: 8
    coarse_scale: 0.125

    downsample3d: False
    downsample_resolution: 30

    # File path substitute:
    path_prefix_substitute_3D_source: null
    path_prefix_substitute_3D_aim: null
    path_prefix_substitute_2D_source: null
    path_prefix_substitute_2D_aim: null

callbacks:
    model_checkpoint:
        _target_: pytorch_lightning.callbacks.ModelCheckpoint
        monitor: cm3degree3
        save_top_k: 5
        save_last: True
        mode: "min"
        dirpath: '${work_dir}/models/checkpoints/${exp_name}'
        filename: '{epoch}-{cm3degree3:.4f}'
    lr_monitor:
        _target_: pytorch_lightning.callbacks.LearningRateMonitor
        logging_interval: 'step'

logger:
    tensorboard:
        _target_: pytorch_lightning.loggers.TensorBoardLogger
        save_dir: '${work_dir}/logs'
        name: ${exp_name}
        default_hp_metric: False

    neptune:
        tags: ["best_model"]
    csv_logger:
        save_dir: "."

hydra:
    run:
      dir: ${work_dir}